# GitHub Issues

Recent issues from the repository (24 total).

## Open Issues (17)

### #2898: LORA adapter with new embedding tokens has size and load issue
**Labels:** No labels | **Created:** 2025-11-05
[View on GitHub](https://github.com/huggingface/peft/issues/2898)

### #2901: AttributeError: 'float' object has no attribute 'meta'
**Labels:** No labels | **Created:** 2025-11-06
[View on GitHub](https://github.com/huggingface/peft/issues/2901)

### #2892: Using trl SFTTrainer creates empty adapter.safetensors file while saving when training with LoRA and Deepspeed Zero3
**Labels:** No labels | **Created:** 2025-11-04
[View on GitHub](https://github.com/huggingface/peft/issues/2892)

### #2884: [Question/Bug] How to safely continue LoRA fine-tuning under DeepSpeed ZeRO-3 (multi-stage training with modules_to_save)
**Labels:** No labels | **Created:** 2025-10-31
[View on GitHub](https://github.com/huggingface/peft/issues/2884)

### #2813: [FEAT] Integrate LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models
**Labels:** No labels | **Created:** 2025-10-06
[View on GitHub](https://github.com/huggingface/peft/issues/2813)

### #2899: Prompt tuning for VLMs like Qwen2.5VL
**Labels:** No labels | **Created:** 2025-11-05
[View on GitHub](https://github.com/huggingface/peft/issues/2899)

### #2651: DoRA slow forward inference
**Labels:** No labels | **Created:** 2025-07-17
[View on GitHub](https://github.com/huggingface/peft/issues/2651)

### #2889: `autocast_adapter_dtype=False` doesn't work when the model is quantized
**Labels:** No labels | **Created:** 2025-11-04
[View on GitHub](https://github.com/huggingface/peft/issues/2889)

### #2880: Bug in fine-tuning for Qwen2.5-VL with LoRA
**Labels:** No labels | **Created:** 2025-10-30
[View on GitHub](https://github.com/huggingface/peft/issues/2880)

### #2886: [FEAT] Integrate QeRL (NVFP4 + LoRA + Noise scheduler) into PEFT
**Labels:** No labels | **Created:** 2025-11-03
[View on GitHub](https://github.com/huggingface/peft/issues/2886)

### #2882: [FEAT] Integrate LoRA-One into PEFT
**Labels:** No labels | **Created:** 2025-10-30
[View on GitHub](https://github.com/huggingface/peft/issues/2882)

### #2885: PeftModel.from_pretrained and merge_and_unload don't work good for me
**Labels:** No labels | **Created:** 2025-11-01
[View on GitHub](https://github.com/huggingface/peft/issues/2885)

### #2873: Can I use Lora fine-tuning twice?
**Labels:** No labels | **Created:** 2025-10-27
[View on GitHub](https://github.com/huggingface/peft/issues/2873)

### #2699: UserWarning: Found missing adapter keys while loading the checkpoint
**Labels:** No labels | **Created:** 2025-08-02
[View on GitHub](https://github.com/huggingface/peft/issues/2699)

### #2310: Comparison of Different Fine-Tuning Techniques for Conversational AI
**Labels:** good first issue, help wanted, contributions-welcome | **Created:** 2025-01-07
[View on GitHub](https://github.com/huggingface/peft/issues/2310)

### #2878: peft " target_modules='all-linear' " have different behavior between x86 and aarch ?
**Labels:** No labels | **Created:** 2025-10-29
[View on GitHub](https://github.com/huggingface/peft/issues/2878)

### #2855: When using the ZeRO3 configuration of deepspeed, the target_parameters cannot obtain the shape of the parameters
**Labels:** No labels | **Created:** 2025-10-21
[View on GitHub](https://github.com/huggingface/peft/issues/2855)


## Recently Closed Issues (7)

### #2881: Bug in prefix tuning of Qwen3-0.6B: group-query attention fix in #1901 still cause error
**Labels:** No labels | **Closed:** 2025-11-05
[View on GitHub](https://github.com/huggingface/peft/issues/2881)

### #2888: Potential remote code execution via untrusted tokenizer_kwargs in PromptEmbedding
**Labels:** No labels | **Closed:** 2025-11-04
[View on GitHub](https://github.com/huggingface/peft/issues/2888)

### #2826: Gradient checkpointing and Activated LoRA models
**Labels:** No labels | **Closed:** 2025-11-03
[View on GitHub](https://github.com/huggingface/peft/issues/2826)

### #2658: check_target_module_exists doesn't accept modules with less than 2 nesting
**Labels:** No labels | **Closed:** 2025-10-30
[View on GitHub](https://github.com/huggingface/peft/issues/2658)

### #2795: Add Support for LoRA-XS
**Labels:** No labels | **Closed:** 2025-10-30
[View on GitHub](https://github.com/huggingface/peft/issues/2795)

### #2777: Update the model config to tie_word_embeddings = False in case of lm_head or embedding layer update
**Labels:** No labels | **Closed:** 2025-10-29
[View on GitHub](https://github.com/huggingface/peft/issues/2777)

### #2877: peft config 'all-linear' include lm_head, is there anyway to remove it ?
**Labels:** No labels | **Closed:** 2025-10-29
[View on GitHub](https://github.com/huggingface/peft/issues/2877)

