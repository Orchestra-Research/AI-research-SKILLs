# peft

Expert guidance for parameter-efficient fine-tuning with PEFT - LoRA, QLoRA, AdaLoRA

## Description

ðŸ¤— PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.

**Repository:** [huggingface/peft](https://github.com/huggingface/peft)
**Language:** Python
**Stars:** 19,994
**License:** Apache License 2.0

## When to Use This Skill

Use this skill when you need to:
- Understand how to use huggingface-peft
- Look up API documentation
- Find usage examples
- Check for known issues or recent changes
- Review release history

## Quick Reference

### Repository Info
- **Homepage:** https://huggingface.co/docs/peft
- **Topics:** adapter, diffusion, llm, parameter-efficient-learning, python, pytorch, transformers, lora, fine-tuning, peft
- **Open Issues:** 57
- **Last Updated:** 2025-11-06

### Languages
- **Python:** 99.6%
- **Makefile:** 0.2%
- **Dockerfile:** 0.1%
- **Cuda:** 0.1%
- **C++:** 0.0%

### Recent Releases
- **v0.17.1** (2025-08-21): 0.17.1
- **v0.17.0** (2025-08-01): 0.17.0: SHiRA, MiSS, LoRA for MoE, and more
- **v0.16.0** (2025-07-03): 0.16.0: LoRA-FA, RandLoRA, CÂ³A, and much more

## Available References

- `references/README.md` - Complete README documentation
- `references/CHANGELOG.md` - Version history and changes
- `references/issues.md` - Recent GitHub issues
- `references/releases.md` - Release notes
- `references/file_structure.md` - Repository structure

## Usage

See README.md for complete usage instructions and examples.

---

**Generated by Skill Seeker** | GitHub Repository Scraper
